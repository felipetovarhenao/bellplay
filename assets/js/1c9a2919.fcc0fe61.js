"use strict";(self.webpackChunkbellplay_docs=self.webpackChunkbellplay_docs||[]).push([[8704],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>i});var a=t(6540);const r={},s=a.createContext(r);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}},9574:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"learning/examples/audiomosaic","title":"9. Audio Mosaicing","description":"An example of basic audio mosaicking in bellplay\\\\~, where a target audio file is reconstructed using segments drawn from a small audio corpus.","source":"@site/docs/learning/examples/audiomosaic.md","sourceDirName":"learning/examples","slug":"/learning/examples/audiomosaic","permalink":"/docs/learning/examples/audiomosaic","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"9. Audio Mosaicing"},"sidebar":"tutorialSidebar","previous":{"title":"8. Temporal Quantization","permalink":"/docs/learning/examples/temporalquantization"},"next":{"title":"10. Envelope-aware Temporal Alignment","permalink":"/docs/learning/examples/temporalalignment"}}');var r=t(4848),s=t(8453);const o={sidebar_position:8,title:"9. Audio Mosaicing"},i="Audio Mosaicing",l={},d=[];function c(e){const n={a:"a",code:"code",h1:"h1",header:"header",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"audio-mosaicing",children:"Audio Mosaicing"})}),"\n",(0,r.jsxs)(n.p,{children:["An example of basic ",(0,r.jsx)(n.a,{href:"https://www.francoispachet.fr/musaicing/",children:"audio mosaicking"})," in ",(0,r.jsx)(n.strong,{children:"bellplay~"}),", where a target audio file is reconstructed using segments drawn from a small audio corpus."]}),"\n",(0,r.jsx)(n.p,{children:"The script below demonstrates how to use onset-based segmentation and multi-dimensional audio features (MFCCs and loudness) to match fragments of a target to acoustically similar units in a pre-analyzed corpus.\nThe target is not reconstructed literally, but instead approximated by audio segments from the corpus."}),"\n",(0,r.jsx)(n.p,{children:"A key aspect of the process is the use of a feature tree, built from the corpus, which allows fast approximate nearest-neighbor retrieval based on the extracted descriptors.\nEach target segment is mapped to its closest match in the corpus and reinserted into the timeline at the corresponding onset, optionally applying a gain envelope and stereo panning."}),"\n",(0,r.jsx)(n.p,{children:"The approach is general and can be adapted to different sources, targets, and features."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bell",metastring:'title="audio_mosaicing.bell" showLineNumbers',children:"## Define features to extract using MFCC and loudness\n$features = mfcc() larm();\n## Collect feature keys for later retrieval\n$featurekeys = for $f in $features collect $f.getkey('output'):-1;\n## Define input files for analysis\n$files = 'badinerie.wav' 'trumpet.wav' 'singing.wav';\n## Onset detection parameters\n$thresh = 0.01;\n$alpha = 0.05;\n$delay = 5;\n## Create onset descriptor with specified parameters\n$onsetdescriptor = onsets(\n    @alpha $alpha\n    @delay $delay\n    @silencethreshold $thresh\n);\n## Function to extract feature values from a buffer\n$buf2features = (\n    $buf -^ $featurekeys -> [for $k in $featurekeys collect $buf.getkey($k) ] \n);\n## List to store features\n$data = null;\n## List to store corresponding audio segments\n$corpus = null;\n## Analyze each file and build corpus\nfor $file in $files do (\n    $buf = importaudio($file);\n    $buf = $buf.analyze($onsetdescriptor);\n    $markers = $buf.getkey('onsets');\n    $segments = $buf.splitbuf(\n        ## split points\n        @split $markers\n        ## mode 2: segments based on pre-computed onsets/markers\n        @mode 2\n    );\n    for $seg in $segments do (\n        ## Extract features for each segment\n        $seg = $seg.analyze($features);\n        ## Append feature vector\n        $data _= $buf2features($seg);\n        ## Append corresponding audio segment\n        $corpus _= $seg\n    ) \n);\n## Create feature tree (e.g., KD-tree) for nearest neighbor search\n$tree = createtree($data);\n## Load and analyze target audio for matching\n$target = importaudio('drums.wav');\n$target = $target.analyze($onsetdescriptor);\n$markers = $target.getkey('onsets');\n## Compute durations between markers\n$markerdurs = x2dx($markers $target.getkey('duration'));\n## Match each segment in the target to corpus using features\nfor $marker in $markers, $dur in $markerdurs do (\n    $segment = $target.setkey('duration', $dur).setkey('offset', $marker);\n    $segment = $segment.analyze($features);\n    $point = $buf2features($segment);\n    $id = querytree($tree, $point);\n    $match = $corpus:$id;\n    ## Transcribe matched segment to aligned position in mix\n    $match.transcribe(\n        @onset $marker\n        @pan 0\n        @gain [0 1 0] [1 0 -0.25] \n    ) \n);\n## Optionally layer original target audio on the right stereo channel\n$target.transcribe(@pan 1);\n## Render and normalize final audio output\nrender(\n    @play 1 @process normalize(-3) \n)\n"})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);