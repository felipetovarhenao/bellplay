"use strict";(self.webpackChunkbellplay_docs=self.webpackChunkbellplay_docs||[]).push([[1507],{66740:e=>{e.exports=JSON.parse('{"tag":{"label":"normalize","permalink":"/docs/tags/normalize","allTagsPath":"/docs/tags","count":15,"items":[{"id":"learning/examples/resynthesis","title":"1. Audio Resynthesis","description":"A simple demonstration of granular quasi-resynthesis via partial tracks, where an sound is used to guide the behavior of sinusoidal grains.","permalink":"/docs/learning/examples/resynthesis"},{"id":"learning/examples/temporalalignment","title":"10. Envelope-aware Temporal Alignment","description":"This example demonstrates how to align the envelopes of different audio samples based on their peak amplitude times.","permalink":"/docs/learning/examples/temporalalignment"},{"id":"learning/tutorials/postprocessing","title":"11. Post-rendering Processing","description":"Sometimes it can be more useful or desirable to apply processing to the entire output, instead of processing each buffer individually.","permalink":"/docs/learning/tutorials/postprocessing"},{"id":"learning/tutorials/analysis","title":"15. Audio Descriptors","description":"One of the core features in bellplay~ is our ability to analyze buffers to extract relevant information.","permalink":"/docs/learning/tutorials/analysis"},{"id":"learning/tutorials/analysismodes","title":"16. Audio Descriptor Modes","description":"When analyzing buffers, we can specify the output format for many of the available audio descriptors.","permalink":"/docs/learning/tutorials/analysismodes"},{"id":"learning/tutorials/features","title":"17. More Audio Features","description":"This tutorial provides an additional example for using buffer analysis features for audio processing.","permalink":"/docs/learning/tutorials/features"},{"id":"learning/tutorials/multirendering","title":"20. Multi-rendering","description":"This tutorial demonstrates a very simple but consequential feature in bellplay~ \u2014 namely, the ability to reuse rendered buffers multiple times to further refine and sculpt the final output into complex and intricate sounds.","permalink":"/docs/learning/tutorials/multirendering"},{"id":"learning/tutorials/sampling","title":"21. Basic Sampling","description":"In bellplay~, the ezsampler function provides a minimal but flexible interface for mapping symbolic pitch and velocity information to audio buffers.","permalink":"/docs/learning/tutorials/sampling"},{"id":"learning/tutorials/importingmidi","title":"22. Importing MIDI","description":"bellplay~ supports importing MIDI files (.mid or .midi) into our scripts, each described as a list of events.","permalink":"/docs/learning/tutorials/importingmidi"},{"id":"learning/tutorials/kdtree","title":"23. Feature-driven Sampling","description":"This tutorial shows how to build k-dimensional trees to efficiently perform feature-based search on buffers.","permalink":"/docs/learning/tutorials/kdtree"},{"id":"learning/tutorials/scoreconfig","title":"24. Score Formatting and Appearance","description":"Depending on what you\'re doing, you might want to have more control over what the score transcription looks like.","permalink":"/docs/learning/tutorials/scoreconfig"},{"id":"learning/examples/feedbacksynth","title":"3. Feedback Synthesis","description":"This code demonstrates a feedback-based synthesis technique, where buffers are routed back into their own processing chain to create a rich, evolving drone.","permalink":"/docs/learning/examples/feedbacksynth"},{"id":"learning/examples/waveshaping","title":"4. Waveshaping Buffers","description":"A basic example of waveshaping in bellplay~, using a randomly generated breakpoint function.","permalink":"/docs/learning/examples/waveshaping"},{"id":"learning/examples/gliss","title":"5. Glissando texture","description":"An example of using time-varying resampling to generate a polyphonic texture.","permalink":"/docs/learning/examples/gliss"},{"id":"learning/examples/audiomosaic","title":"9. Audio Mosaicing","description":"An example of basic audio mosaicking in bellplay\\\\~, where a target audio file is reconstructed using segments drawn from a small audio corpus.","permalink":"/docs/learning/examples/audiomosaic"}],"unlisted":false}}')}}]);