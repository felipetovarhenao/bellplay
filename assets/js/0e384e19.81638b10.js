"use strict";(self.webpackChunkbellplay_docs=self.webpackChunkbellplay_docs||[]).push([[3976],{7879:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"intro","title":"Introduction","description":"What is bellplay~?","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/docs/intro","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"Installation","permalink":"/docs/installation"}}');var r=t(4848),i=t(8453);const o={sidebar_position:1},a="Introduction",l={},c=[{value:"What is <strong>bellplay~</strong>?",id:"what-is-bellplay",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,r.jsxs)(n.h2,{id:"what-is-bellplay",children:["What is ",(0,r.jsx)(n.strong,{children:"bellplay~"}),"?"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"bellplay~"})," is a framework and software for offline algorithmic audio in the ",(0,r.jsx)(n.code,{children:"bell"})," programming language. It is designed to support symbolic and audio workflows within an out-of-the-box, integrated environment."]}),"\n",(0,r.jsxs)(n.p,{children:["At the core of ",(0,r.jsx)(n.strong,{children:"bellplay~"})," is the concept of the ",(0,r.jsx)(n.strong,{children:"buffer"})," \u2014 a structured representation of an audio object, containing both technical metadata (e.g., sampling rate, duration) and optionally, analysis-derived features such as pitch, loudness, or spectral descriptors. These buffers can be generated, processed, and analyzed using ",(0,r.jsx)(n.code,{children:"bell"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["The typical workflow in ",(0,r.jsx)(n.strong,{children:"bellplay~"})," consists of three steps:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generation"}),": Buffers are created using synthesis or sampling."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transcription"}),": Selected buffers are placed on a timeline and visualized in staff notation as note events."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rendering"}),": All transcribed buffers are compiled into a single output buffer, which can itself be re-used within the same script."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Here's a minimal example that synthesizes a 1-second 440 Hz sine tone and renders it:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bell",children:"$buff = cycle(@frequency 440 @duration 1000); ## generation\n$buff.transcribe(); ## transcription\nrender() ## rendering\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"bellplay~"})," supports multi-pass rendering, batch processing, multi-channel audio, symbolic and audio feature extraction, concatenative synthesis, and more. It was originally developed for multimedia and pedagogical use but has since grown into a general-purpose environment for algorithmic composition and sound design."]}),"\n",(0,r.jsxs)(n.p,{children:["If you're new to ",(0,r.jsx)(n.code,{children:"bell"}),", check first the musician-friendly ",(0,r.jsx)(n.a,{href:"https://felipetovarhenao.github.io/bell-tutorials",children:"bell tutorials"}),", then return here for an introduction to ",(0,r.jsx)(n.strong,{children:"bellplay~"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["For questions, feedback, or bug reports, join the ",(0,r.jsx)(n.a,{href:"https://discord.gg/RKZxTwWvxd",children:"community Discord"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);