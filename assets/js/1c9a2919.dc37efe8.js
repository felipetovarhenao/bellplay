"use strict";(self.webpackChunkbellplay_docs=self.webpackChunkbellplay_docs||[]).push([[8704],{8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>o});var a=n(6540);const r={},s=a.createContext(r);function i(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(s.Provider,{value:t},e.children)}},9574:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"learning/examples/audiomosaic","title":"8. Audio Mosaicing","description":"An example of basic audio mosaicking in bellplay~ \u2014 a target audio is reconstructed using a corpus of short audio segments.","source":"@site/docs/learning/examples/audiomosaic.md","sourceDirName":"learning/examples","slug":"/learning/examples/audiomosaic","permalink":"/docs/learning/examples/audiomosaic","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"8. Audio Mosaicing"},"sidebar":"tutorialSidebar","previous":{"title":"7. Temporal Quantization","permalink":"/docs/learning/examples/temporalquantization"},"next":{"title":"9. Envelope-aware Temporal Alignment","permalink":"/docs/learning/examples/temporalalignment"}}');var r=n(4848),s=n(8453);const i={sidebar_position:7,title:"8. Audio Mosaicing"},o="Audio Mosaicing",l={},d=[];function c(e){const t={a:"a",code:"code",h1:"h1",header:"header",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"audio-mosaicing",children:"Audio Mosaicing"})}),"\n",(0,r.jsxs)(t.p,{children:["An example of basic ",(0,r.jsx)(t.a,{href:"https://www.francoispachet.fr/musaicing/",children:"audio mosaicking"})," in ",(0,r.jsx)(t.strong,{children:"bellplay~"})," \u2014 a target audio is reconstructed using a corpus of short audio segments.\nIt outlines the process of setting up parameters for analysis, selecting audio fragments based on feature similarity, and assembling the final output."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bell",metastring:'title="audio_mosaicing.bell" showLineNumbers',children:"## Define the audio features to extract (MFCC and loudness)\n$features = mfcc() larm();\n## Set threshold, alpha value, and delay parameters for onset detection analysis\n$thresh = 0.01;\n$alpha = 0.05;\n$delay = 5;\n## Build a corpus using the specified audio files and feature descriptors\nbuildcorpus(\n    ## Audio sources for corpus\n    @files 'badinerie.wav' 'trumpet.wav' 'singing.wav' \n    ## Extracted features for similarity search\n    @descriptors $features\n    ## Weighting factor for analysis\n    @alpha $alpha\n    ## Silence detection threshold\n    @silencethreshold $thresh\n    ## Delay before onset detection\n    @delay $delay\n);\n## Query the generated corpus to retrieve stored buffer references\n$bufs = query();\n## Extract feature keys from the features list\n$featurekeys = for $f in $features collect $f.getkey('output'):-1;\n## Utility function to extract pre-computed features from input buffer as wrapped list\n$buf2features = (\n    $buf -^ $featurekeys -> [for $k in $featurekeys collect $buf.getkey($k) ] \n);\n## collect relevant features from every buffer\n$data = for $b in $bufs collect $buf2features($b);\n## Build a search tree for efficient retrieval based on extracted features\n$tree = createtree($data);\n## Import a target audio file for analysis\n$target = importaudio('drums.wav');\n## Perform onset detection on the target audio\n$analysis = onsets(\n    ## Weighting factor for analysis\n    @alpha $alpha\n    ## Delay before onset detection\n    @delay $delay\n    ## Silence detection threshold\n    @silencethreshold $thresh\n);\n## Apply the analysis to the target audio buffer\n$target = $target.analyze($analysis);\n## Retrieve detected onset positions from the analysis\n$markers = $target.getkey('onsets');\n## Compute segment durations based on onset markers\n$markerdurs = x2dx($markers $target.getkey('duration'));\n## Loop through detected onset markers and corresponding durations\nfor $marker in $markers, $dur in $markerdurs do (\n    ## Define a segment from the target buffer using detected offset and duration\n    $segment = $target.setkey('duration', $dur).setkey('offset', $marker);\n    ## Analyze the segment with the predefined feature descriptors\n    $segment = $segment.analyze($features);\n    ## Extract features to query tree\n    $point = $buf2features($segment);\n    ## Search the corpus for the best-matching segment's index in data\n    $id = querytree($tree, $point);\n    $match = $bufs:$id;\n    ## Transcribe the matched segment with onset and gain envelope settings\n    $match.transcribe(\n        ## Align match onset with detected marker\n        @onset $marker\n        ## Place match in center of stereo field\n        @pan 0\n        ## Apply gain envelope\n        @gain [0 1 0] [1 0 -0.25] \n    ) \n);\n## Transcribe the original target audio on the right stereo channel\n$target.transcribe(@pan 1);\n## Render the final output with normalization applied\nrender(\n    ## Normalize audio to -3 dB\n    @play 1 @process normalize(-3) \n)\n"})})]})}function u(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);